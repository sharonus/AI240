{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn Exercise   AI240 - 10459\n",
    "## Instructor - Dr. Uma Gajendragadkar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 -\n",
    "First install Numpy Library (if it's not already installed) as it is required by Scikit Learn Library.\n",
    "\n",
    "!pip install numpy\n",
    "\n",
    "If already not installed, then install Scikit Learn Library using below instruction on Jupyter Notebook\n",
    "\n",
    "!pip install scikit-learn\n",
    "\n",
    "Once the installation is complete, you can verify whether Scikit-learn is installed or not by typing the following command:\n",
    "\n",
    "pip show scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: scikit-learn\n",
      "Version: 1.5.1\n",
      "Summary: A set of python modules for machine learning and data mining\n",
      "Home-page: https://scikit-learn.org\n",
      "Author: \n",
      "Author-email: \n",
      "License: new BSD\n",
      "Location: C:\\ProgramData\\anaconda3\\Lib\\site-packages\n",
      "Requires: joblib, numpy, scipy, threadpoolctl\n",
      "Required-by: imbalanced-learn\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 -\n",
    "First import pandas and numpy with below statements import numpy as np import pandas as pd\n",
    "\n",
    "Import the scikit Learn Library datasets with below statement\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 -\n",
    "Load default datsets - the iris dataset with below statetments.\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "The Iris dataset, a classic in machine learning, is a dataset of 150 iris flower measurements, including sepal and petal length and width, used for classification tasks, and is readily available within the scikit-learn library.\n",
    "\n",
    "A dataset is a dictionary-like object that holds all the data and some metadata about the data.The variables of data are called its features. They are also known as predictors or attributes.\n",
    "\n",
    "Feature matrix − It is the collection of features, in case there are more than one.\n",
    "\n",
    "Feature Names − list of names of the features.\n",
    "\n",
    "Response − It is the output variable that basically depends upon the feature variables. They are also known as target, label or output.\n",
    "\n",
    "Response Vector − It is used to represent response column. Generally, we have just one response column.\n",
    "\n",
    "Target Names − It represent the possible values taken by a response vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 -\n",
    "Lets's use IRIS dataset and execute below lines of code and check the output.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "\n",
    "y = iris.target\n",
    "\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(\"Feature names:\", feature_names)\n",
    "\n",
    "print(\"Target names:\", target_names)\n",
    "\n",
    "print(\"\\nFirst 10 rows of X:\\n\", X[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target names: ['setosa' 'versicolor' 'virginica']\n",
      "\n",
      "First 10 rows of X:\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]]\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "print('Feature names:', feature_names)\n",
    "print('Target names:', target_names)\n",
    "print('\\nFirst 10 rows of X:\\n', X[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 -\n",
    "Splitting the dataset Split the dataset into two pieces-a training set and a testing set. Use the training set to train the model and testing set to test the model. Execute below lines of code to split the data into 80:20 ratio . It means 80% data is training data and 20% as testing data.\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "(30, 4)\n",
      "(120,)\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 -\n",
    "Train the Model Use our dataset to train some prediction-model. Scikit-learn has wide range of Machine Learning (ML) algorithms.\n",
    "\n",
    "Here, we are going to use KNN (K nearest neighbors) classifier for sake of learning the scikit Library. We are going to learn classification in detail in coming weeks. Below lines of code are to be executed and are used to make you understand the implementation.\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "classifier_knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "classifier_knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier_knn.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "classifier_knn.fit(X_train, y_train)\n",
    "y_pred = classifier_knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', metrics.accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
